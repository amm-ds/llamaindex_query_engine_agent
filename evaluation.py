import json
import asyncio
from typing import Dict, Any, List, Optional
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)
from datasets import Dataset

from agent_backend import QueryEngineAgent
from configs.config import get_config


class AgentEvaluator:
    """
    A class for evaluating the performance of a QueryEngineAgent using RAGAS metrics.

    This evaluator loads questions and expected answers, generates responses using
    the agent, and evaluates these responses based on various metrics such as
    faithfulness, answer relevancy, context precision, and context recall.
    """

    def __init__(self):
        """
        Initialize the AgentEvaluator with configuration settings and agent backend.

        This constructor sets up the evaluator by loading the configuration from
        the config module and initializing the QueryEngineAgent for processing questions.
        """
        self.config: Dict[str, Any] = get_config()
        self.agent_backend: QueryEngineAgent = QueryEngineAgent()

    async def _async_evaluate(
        self, dataset: Dataset, metrics: List[Any]
    ) -> Dict[str, Any]:
        """
        Asynchronously evaluate the dataset using specified RAGAS metrics.

        Args:
            dataset (Dataset): The dataset to evaluate.
            metrics (List[Any]): List of RAGAS metric functions to apply.

        Returns:
            Dict[str, Any]: Evaluation results for each metric.
        """
        return evaluate(dataset, metrics)

    def _evaluate_answers(
        self,
        questions: List[str],
        expected_answers: List[str],
        responses: List[str],
        contexts: List[List[str]],
    ) -> Dict[str, Any]:
        """
        Evaluate the agent's responses using RAGAS metrics.

        This method prepares the dataset, runs the evaluation, and processes the results.

        Args:
            questions (List[str]): List of questions posed to the agent.
            expected_answers (List[str]): List of expected answers for each question.
            responses (List[str]): List of actual responses generated by the agent.
            contexts (List[List[str]]): List of contexts (retrieved passages) for each question.

        Returns:
            Dict[str, Any]: A dictionary containing overall scores and individual scores for each metric.
        """
        # Ensure all input lists have the same length
        min_length = min(
            len(questions), len(expected_answers), len(responses), len(contexts)
        )
        questions = questions[:min_length]
        expected_answers = expected_answers[:min_length]
        responses = responses[:min_length]
        contexts = contexts[:min_length]

        # Prepare the dataset for RAGAS evaluation
        try:
            dataset = Dataset.from_dict(
                {
                    "question": questions,
                    "answer": responses,
                    "contexts": contexts,
                    "ground_truths": [[answer] for answer in expected_answers],
                }
            )
        except Exception as e:
            print(f"Error creating dataset: {str(e)}")
            print(
                f"Lengths - questions: {len(questions)}, expected_answers: {len(expected_answers)}, "
                f"responses: {len(responses)}, contexts: {len(contexts)}"
            )
            return {"error": "Failed to create dataset for evaluation"}

        # Define the metrics to use
        metrics = [context_precision, context_recall, faithfulness, answer_relevancy]

        # Run the evaluation in a new event loop
        try:
            result = asyncio.run(self._async_evaluate(dataset, metrics))
        except Exception as e:
            print(f"Error during evaluation: {str(e)}")
            return {"error": "Failed to run evaluation"}

        # Process and return the results
        overall_scores = {metric: score for metric, score in result.items()}
        df = result.to_pandas()

        # Extract metrics from the DataFrame
        metric_columns = [
            "context_precision",
            "context_recall",
            "faithfulness",
            "answer_relevancy",
        ]
        metrics_data = df[
            ["question", "ground_truth", "answer"] + metric_columns
        ].to_dict(orient="records")

        # Calculate overall scores
        overall_scores = df[metric_columns].mean().to_dict()

        return {"overall_scores": overall_scores, "individual_scores": metrics_data}

    def _serialize_tool_output(self, output: Any) -> Dict[str, Any]:
        """
        Serialize the output of a tool used by the agent.

        This method converts complex tool output objects into a dictionary format
        that can be easily serialized to JSON.

        Args:
            output (Any): The tool output to serialize.

        Returns:
            Dict[str, Any]: A dictionary containing the serialized tool output.
        """
        # Extract raw output, handling different possible formats
        raw_output = (
            output.raw_output
            if isinstance(output.raw_output, str)
            else (
                output.raw_output.response
                if hasattr(output.raw_output, "response")
                else str(output.raw_output)
            )
        )

        # Extract source nodes if available
        source_nodes = (
            [
                {"id": node.node.id_, "text": node.node.text}
                for node in output.raw_output.source_nodes
            ]
            if hasattr(output.raw_output, "source_nodes")
            else []
        )

        return {
            "content": output.content,
            "tool_name": output.tool_name,
            "raw_input": output.raw_input,
            "raw_output": raw_output,
            "is_error": output.is_error,
            "source_nodes": source_nodes,
        }

    def _serialize_agent_response(self, response: Any) -> Dict[str, Any]:
        """
        Serialize the response generated by the agent.

        This method converts the agent's response object into a dictionary format
        that can be easily serialized to JSON.

        Args:
            response (Any): The agent response to serialize.

        Returns:
            Dict[str, Any]: A dictionary containing the serialized agent response.
        """
        return {
            "response": response.response,
            "sources": [
                self._serialize_tool_output(source) for source in response.sources
            ],
        }

    def _process_agent_responses(self, responses: List[Any]) -> Dict[str, Any]:
        """
        Process and serialize a list of agent responses.

        This method takes a list of raw agent responses and converts them into
        a format suitable for further analysis or storage.

        Args:
            responses (List[Any]): A list of agent responses to process and serialize.

        Returns:
            Dict[str, Any]: A dictionary containing the processed and serialized agent responses.
        """
        data = [self._serialize_agent_response(response) for response in responses]
        return {"agent_responses": data}

    def _save_to_json(self, data: Dict[str, Any], filename: str) -> None:
        """
        Save the provided data to a JSON file.

        Args:
            data (Dict[str, Any]): The data to save to the JSON file.
            filename (str): The name of the file to save the data to.
        """
        with open(filename, "w") as f:
            json.dump(data, f, indent=4)

    def run_evaluation(self) -> None:
        """
        Run the complete evaluation process.

        This method performs the following steps:
        1. Load questions and expected answers from a file.
        2. Generate agent responses for each question.
        3. Evaluate the responses using RAGAS metrics.
        4. Save the evaluation results and processed responses to JSON files.
        """
        # Load questions and expected answers
        with open(self.config["evaluation"]["questions_file"], "r") as f:
            questions_data = json.load(f)

        questions = questions_data["questions"]
        expected_answers = questions_data["expected_answers"]

        responses = []
        contexts = []

        # Generate agent responses for each question
        for q in questions:
            try:
                response = self.agent_backend.process_question(q)
                responses.append(response)
                # Extract context from response sources
                context = [source.content for source in response.sources]
                contexts.append(context)
            except Exception as e:
                print(f"Error processing question '{q}': {str(e)}")
                responses.append(None)
                contexts.append([])

        # Filter out None responses
        valid_responses = [r for r in responses if r is not None]

        # Evaluate answers using RAGAS metrics
        evaluations = self._evaluate_answers(
            questions[: len(valid_responses)],
            expected_answers[: len(valid_responses)],
            [r.response for r in valid_responses],
            contexts[: len(valid_responses)],
        )

        # Save evaluation results
        self._save_to_json(
            evaluations["overall_scores"],
            self.config["evaluation"]["evaluations_output_file"],
        )
        self._save_to_json(
            evaluations["individual_scores"],
            self.config["evaluation"]["individual_scores_output_file"],
        )

        # Process and save agent responses
        processed_responses = self._process_agent_responses(valid_responses)
        self._save_to_json(
            processed_responses,
            self.config["evaluation"]["processed_agent_responses_file"],
        )


if __name__ == "__main__":
    evaluator = AgentEvaluator()
    evaluator.run_evaluation()
